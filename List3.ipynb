{
 "metadata": {
  "name": "",
  "signature": "sha256:b5fc76f073dd78005cc6486e69214dc8f030449584c31dbb17100c2c022feae1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import sklearn.datasets\n",
      "import scipy.stats\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Zadanie 1.1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "    \\def \\ak {a_k^{(i)}}\n",
      "    \\def \\al {a_l^{(i)}}\n",
      "    \\def \\aj {a_j^{(i)}}\n",
      "    \\def \\ok {o_k^{(i)}}\n",
      "    \\def \\ol {o_l^{(i)}}\n",
      "    \\def \\yl {[y^{(i)} = l]}\n",
      "    \\def \\yk {[y^{(i)} = k]}\n",
      "    \\def \\akd {\\partial \\ak}\n",
      "    \\def \\sumKl {\\sum_{l=1}^{K}}\n",
      "    \\def \\sumKj {\\sum_{j=1}^{K}}\n",
      "    \\def \\sumKlk {\\sum_{l=1,l \\neq k}^{K}}\n",
      "    \\frac{\\partial J^{(i)}}{\\akd} = -\\sumKl \\yl \\frac{\\partial \\log \\ol}{\\akd} = \\\\\n",
      "    = -\\sumKl \\yl \\frac{1}{\\ol} \\frac{\\partial \\ol}{\\akd} = \\\\\n",
      "    = -\\sumKl \\yl \\frac{1}{\\ol} \\frac{\\frac{\\partial e^{\\al}}{\\akd} \\sumKj e^{\\aj} - \\frac{\\partial \\sumKj e^{\\aj}}{\\akd} e^{\\al}}{(\\sumKj e^{\\aj})^2} = \\\\\n",
      "    = -\\sumKlk \\yl \\frac{1}{\\ol} \\frac{\\frac{\\partial e^{\\al}}{\\akd} \\sumKj e^{\\aj} - \\frac{\\partial \\sumKj e^{\\aj}}{\\akd} e^{\\al}}{(\\sumKj e^{\\aj})^2} - \\yk \\frac{1}{\\ok} \\frac{\\frac{\\partial e^{\\ak}}{\\akd} \\sumKj e^{\\aj} - \\frac{\\partial \\sumKj e^{\\aj}}{\\akd} e^{\\ak}}{(\\sumKj e^{\\aj})^2} = \\\\\n",
      "    = -\\sumKlk \\yl \\frac{1}{\\ol} \\frac{-e^{\\ak} e^{\\al}}{(\\sumKj e^{\\aj})^2} - \\yk \\frac{1}{\\ok} \\frac{e^{\\ak} \\sumKj e^{\\aj} - e^{\\ak} e^{\\ak}}{(\\sumKj e^{\\aj})^2} = \\\\\n",
      "    = -\\sumKlk \\yl \\frac{\\sumKj e^{\\aj}}{e^{\\al}} \\frac{-e^{\\ak} e^{\\al}}{(\\sumKj e^{\\aj})^2} - \\yk \\frac{\\sumKj e^{\\aj}}{e^{\\ak}} \\frac{e^{\\ak} \\sumKj e^{\\aj} - e^{\\ak} e^{\\ak}}{(\\sumKj e^{\\aj})^2} = \\\\\n",
      "    = -\\sumKlk \\yl \\frac{1}{e^{\\al}} \\frac{-e^{\\ak} e^{\\al}}{\\sumKj e^{\\aj}} - \\yk \\frac{1}{e^{\\ak}} \\frac{e^{\\ak} \\sumKj e^{\\aj} - e^{\\ak} e^{\\ak}}{\\sumKj e^{\\aj}} = \\\\\n",
      "    = -\\sumKlk \\yl \\frac{-e^{\\ak}}{\\sumKj e^{\\aj}} - [y=k] \\frac{\\sumKj e^{\\aj} - e^{\\ak}}{\\sumKj e^{\\aj}} = \\\\\n",
      "    = \\sumKlk \\yl \\ok - \\yk (1 - \\ok) = \\\\\n",
      "    = \\sumKlk \\yl \\ok + \\yk \\ok - \\yk = \\\\\n",
      "    = \\sumKl \\yl \\ok - \\yk = \\\\\n",
      "    = \\ok - \\yk\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Zadanie 1.2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "nie zmieni si\u0119\n",
      "$$\n",
      "    o_k = \\frac{e^{a_k}}{\\sum_{j=1}^{K} e^{a_j}} \\\\\n",
      "    b_l = a_l + c \\\\\n",
      "    o'_k = \\frac{e^{b_k}}{\\sum_{j=1}^{K} e^{b_j}} = \\frac{e^{a_k + c}}{\\sum_{j=1}^{K} e^{a_j + c}} = \\frac{e^{a_k} e^{c}}{\\sum_{j=1}^{K} e^{a_j} e^{c}} = \\frac{e^{c} e^{a_k}}{e^{c} \\sum_{j=1}^{K} e^{a_j}} = \\frac{e^{a_k}}{\\sum_{j=1}^{K} e^{a_j}} = o_k\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Zadanie 1.3"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# via: http://deeplearning.stanford.edu/wiki/index.php/Exercise:Softmax_Regression\n",
      "def get_ground_truth(values):\n",
      "    indices = values.flatten() - 1\n",
      "    data = numpy.ones(len(indices))\n",
      "    indptr = numpy.arange(len(indices) + 1)\n",
      "    \n",
      "    return scipy.sparse.csr_matrix((data, indices, indptr)).todense()\n",
      "\n",
      "def softmax_func1(data):\n",
      "    data = numpy.array(data, dtype=numpy.float64).reshape((len(data)))\n",
      "    result = numpy.exp(data)\n",
      "    return result / numpy.sum(result)\n",
      "\n",
      "def softmax_func(data):\n",
      "    data = numpy.array(data, dtype=numpy.float64).reshape((len(data), len(data[0])))\n",
      "    result = numpy.exp(data)\n",
      "    return result / numpy.sum(result, axis=1)[:, numpy.newaxis]\n",
      "\n",
      "def softmax_cost_(theta, data, target):\n",
      "    theta = numpy.array(theta, dtype=numpy.float64).reshape((max(target), len(data[0])))\n",
      "    #numerical = numerical_gradient(lambda p: (softmax_cost(p, data, target), 0), list(theta.flatten()))\n",
      "    target = numpy.array(target, dtype=numpy.float64).reshape((len(target), 1))\n",
      "    data = numpy.array(data, dtype=numpy.float64).reshape((len(data), len(data[0])))\n",
      "    ground_truth = get_ground_truth(target)\n",
      "    analytical = numpy.array(transpose(softmax_func(data.dot(transpose(theta))) - ground_truth).dot(data) / len(data)).flatten()\n",
      "    return analytical\n",
      "\n",
      "def softmax_cost(theta, data, target):\n",
      "    theta = numpy.array(theta, dtype=numpy.float64).reshape((max(target), len(data[0])))\n",
      "    target = numpy.array(target, dtype=numpy.float64).reshape((len(target), 1))\n",
      "    data = numpy.array(data, dtype=numpy.float64).reshape((len(data), len(data[0])))\n",
      "    ground_truth = get_ground_truth(target)                  \n",
      "    return -numpy.sum(numpy.multiply(ground_truth, numpy.log(softmax_func(data.dot(transpose(theta)))))) / len(data)\n",
      "\n",
      "def softmax(theta, data, target):\n",
      "    return softmax_cost(theta, data, target), softmax_cost_(theta, data, target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def BGD(function, point, step, diff):\n",
      "    currentdiff = 1000000000.0\n",
      "    current, gradient = function(point)\n",
      "    iterations = 0\n",
      "    while currentdiff >= diff:\n",
      "        point -= step * gradient\n",
      "        nextcurrent, gradient = function(point)\n",
      "        currentdiff = numpy.abs(current - nextcurrent)\n",
      "        current = nextcurrent\n",
      "        iterations += 1\n",
      "    \n",
      "    return point, iterations\n",
      "\n",
      "def numerical_gradient(function, point, EPS=10e-7):\n",
      "    result = numpy.array(\n",
      "        map(\n",
      "            lambda (n, x):\n",
      "                1.0 * (\n",
      "                        function(point[:n] + [x + EPS,] + point[n + 1:])[0]\n",
      "                    -   function(point[:n] + [x - EPS,] + point[n + 1:])[0]\n",
      "                ) / (2.0 * EPS), enumerate(point)\n",
      "        )\n",
      "    )\n",
      "    \n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.optimize\n",
      "iris = sklearn.datasets.load_iris()\n",
      "data = numpy.insert(iris.data, 0, 1., axis=1)\n",
      "target = iris.target + 1\n",
      "indexes = range(len(data))\n",
      "tests = len(data) / 3\n",
      "\n",
      "TESTS = 100\n",
      "errors = 0\n",
      "for t in range(TESTS):\n",
      "    random.shuffle(indexes)\n",
      "    training = data[indexes[tests:]]\n",
      "    classification = target[indexes[tests:]]\n",
      "    #theta = scipy.optimize.fmin_l_bfgs_b(lambda p: (softmax_cost(p, training, classification), numerical_gradient(lambda x: (softmax_cost(x, training, classification), 0), list(p))), [0.] * 15)[0]\n",
      "    theta = scipy.optimize.fmin_l_bfgs_b(lambda p: softmax(p, training, classification), [0.] * 15)[0]\n",
      "    #theta = BGD(lambda p: softmax(p, training, classification), [0.] * 15, 10e-5, 10e-5)[0]\n",
      "   \n",
      "    theta = numpy.array(theta, dtype=numpy.float64).reshape((max(classification), len(training[0])))\n",
      "    \n",
      "    guess = softmax_func(data[indexes[:tests]].dot(transpose(theta)))\n",
      "    errors += numpy.count_nonzero(numpy.argmax(guess, axis=1) + 1 != target[indexes[:tests]])\n",
      "    \n",
      "    print 100.0 * errors / (tests * (t + 1)), '\\r',\n",
      "    \n",
      "print 100.0 * errors / (tests * TESTS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "16.0 \r",
        "11.0 \r",
        "8.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "9.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "7.2 \r",
        "6.33333333333"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "6.28571428571 \r",
        "6.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "6.0 \r",
        "5.6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "5.27272727273 \r",
        "5.66666666667"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "5.53846153846 \r",
        "5.14285714286"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "5.06666666667"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "4.75 \r",
        "4.47058823529 \r",
        "4.22222222222"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "4.10526315789 \r",
        "4.2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "4.19047619048"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "4.09090909091 \r",
        "4.17391304348"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "4.0 \r",
        "4.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "4.07692307692"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "4.0 \r",
        "4.07142857143"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.93103448276 \r",
        "3.8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.74193548387 \r",
        "3.625"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.69696969697"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.64705882353 \r",
        "3.65714285714"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.77777777778"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.72972972973"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.84210526316"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.84615384615"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.85 \r",
        "3.75609756098"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.66666666667 \r",
        "3.72093023256"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.68181818182"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.64444444444 \r",
        "3.65217391304"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.57446808511 \r",
        "3.58333333333"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.63265306122 \r",
        "3.72 \r",
        "3.64705882353"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.76923076923"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.7358490566 \r",
        "3.81481481481"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.81818181818"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.75 \r",
        "3.68421052632"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.68965517241"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.76271186441"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.80327868852 \r",
        "3.8064516129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.77777777778 \r",
        "3.78125 \r",
        "3.75384615385"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.84848484848 \r",
        "3.85074626866"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.85294117647"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.88405797101"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.85714285714 \r",
        "3.88732394366"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.83333333333 \r",
        "3.78082191781"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.78378378378"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.76"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.73684210526 \r",
        "3.76623376623"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.82051282051"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.84810126582 \r",
        "3.825"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.85185185185"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.80487804878 \r",
        "3.75903614458 \r",
        "3.7619047619"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.76470588235"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.76744186047 \r",
        "3.79310344828"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.81818181818"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.8202247191"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.8 \r",
        "3.82417582418"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.84782608696 \r",
        "3.89247311828"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.87234042553"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.83157894737 \r",
        "3.8125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.85567010309 \r",
        "3.87755102041"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.85858585859"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "3.86 \r",
        "3.86\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:33: RuntimeWarning: divide by zero encountered in log\n",
        "-c:33: RuntimeWarning: invalid value encountered in multiply\n",
        "-c:16: RuntimeWarning: overflow encountered in exp\n",
        "-c:17: RuntimeWarning: invalid value encountered in divide\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Zadanie 2.1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M = 7\n",
      "N = 2\n",
      "\n",
      "L = 3\n",
      "K = 5\n",
      "\n",
      "def apply_simplenet(hidden_matrix, hidden_bias, output_matrix, output_bias, data):\n",
      "    complete_input = data.dot(hidden_matrix) + hidden_bias\n",
      "    hidden_activation = numpy.tanh(complete_input)\n",
      "    complete_output = hidden_activation.dot(output_matrix) + output_bias\n",
      "    return softmax_func(complete_output)\n",
      "\n",
      "def simplenet_cost(hidden_matrix, hidden_bias, output_matrix, output_bias, data, target):\n",
      "    complete_input = data.dot(hidden_matrix) + hidden_bias\n",
      "    hidden_activation = numpy.tanh(complete_input)\n",
      "    complete_output = hidden_activation.dot(output_matrix) + output_bias\n",
      "    ground_truth = get_ground_truth(target)\n",
      "    return -numpy.sum(numpy.multiply(ground_truth, numpy.log(softmax_func(complete_output)))) / M\n",
      "\n",
      "def simplenet_cost_(hidden_matrix, hidden_bias, output_matrix, output_bias, data, target):\n",
      "    complete_input = data.dot(hidden_matrix) + hidden_bias\n",
      "    hidden_activation = numpy.tanh(complete_input)\n",
      "    complete_output = hidden_activation.dot(output_matrix) + output_bias\n",
      "    ground_truth = get_ground_truth(target)\n",
      "\n",
      "    hidden_matrix_gradient = transpose(data).dot(numpy.multiply((softmax_func(complete_output) - ground_truth).dot(transpose(output_matrix)), (1 - hidden_activation**2))) / M\n",
      "    hidden_bias_gradient = numpy.sum((softmax_func(complete_output) - ground_truth).dot(transpose(output_matrix)), axis=0) / M\n",
      "    output_matrix_gradient = transpose(hidden_activation).dot(softmax_func(complete_output) - ground_truth) / M    \n",
      "    output_bias_gradient = numpy.sum(softmax_func(complete_output) - ground_truth, axis=0) / M\n",
      "    return hidden_matrix_gradient, hidden_bias_gradient, output_matrix_gradient, output_bias_gradient\n",
      "\n",
      "def simplenet(hidden_matrix, hidden_bias, output_matrix, output_bias, data, target):\n",
      "    return simplenet_cost(hidden_matrix, hidden_bias, output_matrix, output_bias, data, target), simplenet_cost_(hidden_matrix, hidden_bias, output_matrix, output_bias, data, target)\n",
      "\n",
      "def train_fun(theta, data, target):\n",
      "    error, gradient = simplenet(*(decode_theta(theta) + [data, target]))\n",
      "    return error, encode_theta(*gradient)\n",
      "\n",
      "def encode_theta(hidden_matrix, hidden_bias, output_matrix, output_bias):\n",
      "    result = numpy.array(numpy.hstack([hidden_matrix.ravel(), hidden_bias.ravel(), output_matrix.ravel(), output_bias.ravel()]), copy=True).flatten()\n",
      "    return result\n",
      "\n",
      "def decode_theta(theta):\n",
      "    return [\n",
      "        numpy.array(theta[:N*L], dtype=numpy.float64).reshape((N, L)),\n",
      "        numpy.array(theta[N*L:N*L+L], dtype=numpy.float64).reshape((1, L)),\n",
      "        numpy.array(theta[N*L+L:N*L+L+L*K], dtype=numpy.float64).reshape((L, K)),\n",
      "        numpy.array(theta[N*L+L+L*K:N*L+L+L*K+K], dtype=numpy.float64).reshape((1, K)),\n",
      "    ]\n",
      "\n",
      "Wh = numpy.array([[numpy.random.normal(0, 0.2) for j in range(L)] for i in range(N)], dtype=numpy.float64).reshape((N, L))\n",
      "bh = numpy.array([0]*L, dtype=numpy.float64).reshape((1, L))\n",
      "Wo = numpy.array([[numpy.random.normal(0, 0.2) for j in range(K)] for i in range(L)], dtype=numpy.float64).reshape((L, K))\n",
      "bo = numpy.array([0]*K, dtype=numpy.float64).reshape((1, K))\n",
      "X = numpy.array([[i*M + j for j in range(M)] for i in range(N)], dtype=numpy.float64).reshape((M, N))\n",
      "Y = numpy.array([numpy.random.choice(range(1, K + 1)) for _ in range(M)], dtype=numpy.float64).reshape((M, 1))\n",
      "\n",
      "theta = scipy.optimize.fmin_l_bfgs_b(lambda p: train_fun(p, X, Y), encode_theta(Wh, bh, Wo, bo))[0]\n",
      "print train_fun(theta, X, Y)\n",
      "print Y\n",
      "print numpy.argmax(apply_simplenet(*(decode_theta(theta) + [X])), axis=1) + 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(0.36935739529407829, array([  1.34510101e-06,   2.09429959e-01,  -4.37783459e-02,\n",
        "        -3.39849076e-06,   2.13425825e-01,  -4.86226356e-02,\n",
        "        -2.09417920e-02,  -5.79980006e-03,  -1.13933594e-01,\n",
        "         6.80591219e-03,  -6.50912532e-04,  -5.06425350e-05,\n",
        "        -9.80625015e-03,   3.70189303e-03,   1.87666631e-03,\n",
        "        -3.53104674e-03,  -3.90273588e-05,   1.05922132e-02,\n",
        "        -8.89880542e-03,  -1.92167110e-02,   1.70524180e-02,\n",
        "         5.08300001e-05,  -2.15144217e-03,   4.26490512e-03,\n",
        "        -6.80591219e-03,   6.50912558e-04,  -5.08181560e-05,\n",
        "         9.83678364e-03,  -3.63096585e-03]))\n",
        "[[ 3.]\n",
        " [ 4.]\n",
        " [ 5.]\n",
        " [ 4.]\n",
        " [ 4.]\n",
        " [ 2.]\n",
        " [ 1.]]\n",
        "[3 4 4 4 4 2 1]\n"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Zadanie 2.2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Zadanie 2.3"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "XOR"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Zadanie 2.4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "IRYSY"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Zadanie 2.5"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "MNIST"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Zadanie 2.6"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a) parametryczne\n",
      "b) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}